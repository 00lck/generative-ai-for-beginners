# 生成系 AI の責任ある使用

[![生成系 AI の責任ある使用](./images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)]() 

> **ビデオは近日公開予定**

AI、特に生成系 AI に魅了されるのは簡単ですが、それをどのように責任を持って運用するかを考える必要があります。出力が、公正で害が無いことをどう保証するかなど、考慮すべき点があります。この章では、そのようなコンテキストや考慮すべき点、そして AI の使用に対して改善するための建設的な方法を提供することに焦点を当てます。

## はじめに

このレッスンでは以下の内容を取り上げます：  

- 生成系 AI アプリケーションを構築する際に、なぜ責任ある AI を優先すべきか
- 責任ある AI の核心的な原則と、それらが生成系 AI とどのように関連するか
- 責任ある AI の原則を戦略とツールで実践する方法

## 学習目標  

このレッスンを完了すると、以下のことが理解できるようになります：  

- 生成系 AI アプリケーションを構築する際に、責任ある AI の重要性
- 生成系 AI アプリケーションを構築する際に、責任ある AI の核心的な原則をいつ、どのように考慮し適用するか
- 責任ある AI の概念を実践に移すために利用可能なツールと戦略

## 責任のある AI の原則

生成系 AI に対する興奮は、今までにないほど高まっています。この興奮は、新しい開発者、注目、資金をこの分野にもたらしています。これは、生成系 AI を使用して製品や、新しい企業を創業する人々にとって非常にプラスですが、同時に責任を持って進めることも重要です。  

このコースでは、スタートアップと AI を利用した教育製品の実装に焦点を当てます。公平性、包括性、信頼性・安全性、セキュリティ・プライバシー、透明性、説明責任といった責任ある AI の原則に基づいて、これらが私たちの製品における生成系 AI の使用と、どのように関連するかを考えます。

## 責任のある AI を優先すべき理由

製品を実装する際、利用者にとって最善の利益を心に留めた、人間中心のアプローチを取ることが、最良の結果につながります。  

生成系 AI の特徴は、利用者に役立つ回答、情報、ガイダンス、コンテンツを生成する力にあります。これは多くの手作業が不要で実行できるため、非常に好意的な結果を生む可能性があります。しかし、適切な計画と戦略がなければ、残念ながら利用者、製品、そして社会全体に有害な結果をもたらすこともあります。  

下記に、潜在的に問題のある結果を出力する例（すべてではありません）を挙げます。

### Hallucinations (幻覚)

「Hallucinations (幻覚)」とは、LLM が完全に無意味な内容や、他の情報に基づいて事実と異なる内容を生成する際に使用する用語です。

例えば、スタートアップがモデルに対して、学生から歴史的な質問を問い合わせできる機能を実装したとします。生徒が「タイタニック号の唯一の生存者は誰でしたか？」と質問します。

モデルは以下のような回答を生成します：

![Prompt saying "Who was the sole survivor of the Titanic"](../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> *(Source: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))*

これは非常に自信に満ちて、詳しい回答内容です。しかし残念ながら、これは間違っています。少し調べれば、タイタニック号の生存者が一人以上いたことがわかります。このトピックについて調査を始めたばかりの学生にとって、この答えは疑う余地はなく、説得力があるため事実として受け入れてしまうかもしれません。こうした結果から、AI システムを信頼できなくなり、スタートアップの評判に悪影響を及ぼす可能性があります。

LLM の各イテレーションで、Hallucinations (幻覚)を最小限に抑えパフォーマンスの向上を見ます。ただ、この改善があったとしても、アプリケーションの開発者やユーザーは、こうした制限を意識し続ける必要があります。

### 有害なコンテンツ

前のセクションで、LLM が不正確または無意味な回答を生成する場合について触れました。もう一つ注意すべきリスクは、モデルが有害なコンテンツを回答する場合です。  

有害なコンテンツとは以下のようなものです：  

- 自傷行為や特定のグループへ危害を助長、または奨励する指示を与えること
- 憎悪や侮辱的な内容
- あらゆる種類の攻撃や暴力行為の計画を指導すること
- 違法なコンテンツを探す方法や、違法行為を犯す方法について指導すること
- 性的に露骨な内容を表示すること

スタートアップでは、このようなコンテンツが学生に見られないようにするために、適切なツールと戦略を確実に導入したいと考えています。  

### 公平性の欠如

公平性とは、「AI システムが偏見や差別を持たず、すべての人を公平かつ平等に扱うこと」を意味します。生成系 AI の世界では、疎外されたグループの排他的な世界観がモデルの出力によって強化されないようにしたいと考えています。

このような公平性が欠如した出力は、ユーザーにとってポジティブな製品体験を実装する上で破壊的であり、さらに社会に害を及ぼすことにもなります。アプリケーションの開発者として、生成系 AI を用いたソリューションを構築する際には、常に幅広く多様なユーザーベースを念頭に置くべきです。

## 責任ある生成系 AI の使用方法

責任ある生成系 AI の重要性を認識した上で、責任を持って AI ソリューションを構築するために取り組むべき4つのステップを見ていきましょう。

![Mitigate Cycle](./images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### 潜在的な危害の測定

ソフトウェアテストでは、アプリケーションに対するユーザーの期待する行動をテストします。同様に、ユーザーが最も使用する可能性が高い多様なプロンプトをテストすることは、潜在的な害を測定する良い方法です。  

スタートアップが教育関連の製品を実装しているので、教育関連のプロンプトのリストを準備すると良いでしょう。これには、特定の科目、歴史的事実、学生生活に関するプロンプトなどが含まれるかもしれません。

### 潜在的な危害の軽減

モデルとその応答によって引き起こされる潜在的な危害を防ぐ、または制限する方法を見つける時が来ました。これを4つの異なる層で考えます。

![Mitigation Layers](./images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **モデル**。適切なユースケースに沿ったモデルを選択します。GPT-4 のような大規模で複雑なモデルは、小規模で特定のユースケースに適用した場合、有害なコンテンツのリスクを高めることがあります。トレーニング・データを使ってモデルをファイン・チューニングすることで、有害なコンテンツのリスクを減らすこともできます。

- **安全システム**。安全システムとは、モデルを提供するプラットフォーム上で危害を軽減するためのツールと設定のツールです。例えば、Azure OpenAIサービスのコンテンツフィルタリングシステムがあります。システムは脱獄攻撃やボットからのリクエストなどの望ましくない活動も検出するべきです。

- **メタ・プロンプト**。メタプロンプトとグラウンディングは、特定の行動や情報に基づいてモデルを指示または制限する方法です。これには、システムの入力を使ってモデルの特定の制限を定めることが含まれます。さらに、システムの範囲やドメインに関連性の高い出力を提供することも含まれます。

Retrieval Augmented Generation（RAG）のような技術を使って、モデルが信頼できる情報源からのみ情報を引き出すようにすることもできます。このコースの後半には、検索アプリケーションの構築に関するレッスンがあります。

- **ユーザーエクスペリエンス**。最後の層は、ユーザーが何らかの方法でアプリケーションのインターフェースを通じてモデルと直接対話する場所です。このようにして、UI/UX を設計し、モデルに送信する入力の種類やユーザーに表示されるテキストや画像を制限することができます。AI　アプリケーションを展開する際には、生成系　AI　アプリケーションができることとできないことについて透明性を持って伝える必要があります。

[AI アプリケーションの UX デザイン](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)に特化したレッスンがあります。

- **モデルを評価**。LLM（大規模言語モデル）を扱うことは、モデルが訓練されたデータを常に制御できないため、挑戦的な場合があります。それでも、モデルのパフォーマンスと出力を常に評価するべきです。出力の精度、類似性、根拠、関連性を測定することは依然として重要で、これにより利害関係者とユーザーに透明性と信頼を提供するのに役立ちます。  

### 責任ある生成系　AI　ソリューションを運用する

AI　アプリケーションを取り巻く運用実践を構築することが最終段階です。これには、法務やセキュリティなどの他の部門と協力し、すべての規制ポリシーに準拠していることを確認することが含まれます。リリース前には、配信、インシデントの処理、ロールバックに関する計画を立てて、ユーザーへの危害が拡大しないようにすることも重要です。

## ツール

責任ある　AI　ソリューションを開発する作業は大変かもしれませんが、その努力は十分に価値があります。生成系　AI　の分野が成長するにつれて、開発者が責任をワークフローに効率的に統合するためのツールが成熟していきます。例えば、[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)は、APIリクエストを通じて有害なコンテンツや画像を検出するのに役立ちます。

## 知識チェック

責任ある　AI　の使用を確保するために注意すべきことは何ですか？  

1. 回答が正しいこと。  
2. AI が犯罪目的で使用されないように有害な使用を防ぐこと
3. AI が偏見や差別から自由であることを保証すること

A: 2 と 3 が正解です。責任あるAIは、有害な影響や偏見を軽減する方法を考慮するのに役立ちます。  

## 🚀 Challenge

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)について調べ、自分の用途に適用できるものを見つけてください。  

## お疲れ様でした! 次のレッスンを続ける

このレッスン終了後、[生成系 AI 学習コレクション](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)をチェックして、生成系 AI の知識をさらに深めましょう。  

レッスン 4 では、[プロンプト・エンジニアリングの基本](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)について学びます！
