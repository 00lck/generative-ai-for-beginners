<!--
GUIDING THEME:
This lesson should answer the question:
"If I were building an education AI startup, how would prompt-engineering help me?"

INTRODUCTION:
Identify 3 core concepts to teach.
Identify 3 learning goals to achieve.

CODE CHALLENGE:
If provided, should have an education focus - help show how the concepts can be applied to make the lives of teachers and students easier.
-->

So far, we've learned the following core concepts:
 - **Generative AI** - is a category of artificial intelligence capable of _generating new content based on pre-trained models_ - in response to a natural language input or "prompt".
 - **Large Language Models (LLM)** - are a type of generative AI trained on massive quantities of text data to execute natural language processing (NLP) tasks at scale. 
 
We've heard of popular LLMs like [GPT-4](https://openai.com/gpt-4) (OpenAI), [BERT](https://github.com/google-research/bert) (Google) and [Llama-2](https://ai.meta.com/llama/) (Meta). And we've seen LLMs power _enterprise-grade applications_ like [GitHub Copilot](https://github.com/features/copilot) which is based on the [OpenAI Codex model](https://openai.com/blog/openai-codex) and built [in partnership with GitHub](https://github.blog/2023-05-17-inside-github-working-with-the-llms-behind-github-copilot/) to generate code and support other code-related tasks, driven by user prompts.

However, the rapid growth and adoption of generative AI also identifies two key challenges:
 - **LLMs are _stochastic_ in nature.** The same prompt may have different outcomes with different LLMs - and may even generate slightly diffetent outcomes on repeating the prompt with the same LLM.
 - **LLMs can _hallucinate_ responses.** LLMs use "pre-trained models", limiting their core knowledge to the training data. Prompts that explore questions outside that scope (e.g., more recent) can result in unexpected responses that are inaccurate, confusing, or even contradictory to well-known facts.
 
This has led to a new field of technology focused on _guiding_ LLMs with more effective prompt design that can reduce or mitigate some of these effects. This new field is popularly known as _prompt enginering_.

## Introduction

In this lesson, we'll define prompt engineering in more detail and learn _why_ prompt engineering matters. Then, we'll explore _prompt examples_ in real-world applications to get a sense of how current LLM features and challenges influence responses. Finally, we'll discuss _prompt engineering techniques_ that can be used to improve the relevance and consistency of prompt-driven responses. We'll focus on basic techniques and best practices in this lesson - and set the stage for more _advanced_ techniques to be discussed in the next lesson.

## Learning Goals

By the end of this lesson you will be able to:
 - Describe what prompt engineering is - and why it matters.
 - Discuss real-world examples of prompt usage - and identify relevant problems.
 - Apply popular prompt engineering techniques - and observe the impact on responses.
 - Learn to build & validate OpenAI prompts in a GitHub Codespaces enabled Jupyter Notebook.

More importantly, you should be able to _apply these learnings_ to your education AI startup and be able to answe the question: _How does prompt engineering help me deliver a better experience to students, educators, adminstrators and other users in my education startup_.