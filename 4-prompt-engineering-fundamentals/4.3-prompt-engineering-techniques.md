<!--
LESSON TEMPLATE:
This unit should cover core concept #1.
Reinforce the concept with examples and references.

CONCEPT #3:
Prompt Engineering Techniques.
What are some basic techniques for prompt engineering?
Illustrate it with some exercises.
-->

The previous unit showed various examples of prompts and illustrated how the responses can vary based on factors like the LLM used, the prompt design (and constraints) - and the level of detail (context) available for the application.

They demonstrated _why_ we need prompt engineering. So how do we actually get started engineering better prompts?

## 4.3.1 Prompt Engineering Mindset

At a high level, prompt engineering requires a combination of _context understanding_ and _creative thinking_. There are 3 factors to keep in mind:
 
 1. **Domain Understanding Matters.** To craft prompts that are more relevant to _your application_, you need to use deep knowledge of the application domain to set relevant context. For instance, use _system prompts_ to specify a setting or persona that reflects your thinking. Or, use templates or prefixes for _user prompts_ to guide the LLM into delivering responses that follow a required pattern or return a desired format.
 2. **Model Understanding Matters.** Use knowledge about the strengths and weaknesses of the specific LLM to _tune_ prompts to optimize response relevance or correctness. For instance, a model that is trained for NLP tasks like summarization may be less optimal when asked to generate code. Similarly, an OpenAI model may offer different _hyperparameter tuning_ options or features (e.g., stop-keywords) than a Llama-2 model. Understanding the differences can help create more universal-vs-personalized prompts.
 3. **Iterate and Validate.** Prompt engineering is more art than science at this stage. Use a trial-and-error approach to fine-tuning your prompts. Apply different techniques _and observe the changes to validate effects_. Refine prompts further _using your own intuition_ till the outcome meets your expectations. And try to identify criteria that can be used to determine _when_ to stop iterating.

So what are some specific examples of techniques we can start with _for basic prompt engineering_? Let's look at the [best practices recommended by the OpenAI team](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api) - though described for Open AI, they can be applied to other LLMs as well.
 * _Use the latest models._ Not only does every generation of the LLM improve its own accuracy and performance, it is also likely to be trained on more recent data - improving its relevance to current scenarios.
 * _Separate instructions from context_. For instance, we can use a _System_ prompt to provide context for the model, and a _User_ prompt to provide instructions for the output response. The LLM may also define _special tokens_ you can use as delimiters for these prompts.
 * _Be specific and clear_. Try to provide more details about the desired context, outcome, length, format, style etc. Not only will this result in responses that meet your needs, they are also likely to be more consistent.
 * _Use more examples_. Models may respond better to a "show and tell" approach. Start with a `zero-shot` approach where you give it an instruction (but no examples) then try `few-shot` as a refinement, providing a few examples of the desired output.
 * _Use leading words to guide model_. Nudge it towards a desired outcome by giving it some leading words or phrases that it can use as a starting point for the response.

There are more advanced techniques that we will talk about in the next lesson - including **Chain of Thought Prompting**, **Prompt Templating** and more.

## Prompt Engineering Exercise

Let's try a simple exercise that demonstrates the value of providing both _context_ (system prompt) and _instructions_ (user prompt) to the model.

![](./assets/prompt-engineer-1.png)

![](./assets/prompt-engineer-2.png)

![](./assets/prompt-engineer-3.png)

## Prompt Engineering: Hallucination

Let's try a simple exercise that demonstrates the challenges of _prompt hallucination_ by asking the model a question that can only be answered by data after 2021 (the model's cutoff date).

![](./assets/prompt-engineer-4.png)

![](./assets/prompt-engineer-5.png)

![](./assets/prompt-engineer-6.png)

