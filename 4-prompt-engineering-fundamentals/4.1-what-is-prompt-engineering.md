<!--
LESSON TEMPLATE:
This unit should cover core concept #1.
Reinforce the concept with examples and references.

CONCEPT #1:
Prompt Engineering.
Define it and explain why it is needed.
-->

In this lesson, we'll try to answer two main questions:
 - What is Prompt Engineering?
 - Why do we need Prompt Engineering?

## 4.1.1 What is Prompt Engineering?

Prompt engineering is the process of _designing and optimizing prompts_ for Generative AI models, for more relevant and reliable application experiences. It is an emerging field of study driven by the rapid adoption of LLMs in enterprise-scale applications. Prompt engineering has multiple facets:
 - Prompt _design_ which we can think of as the process of "writing good prompts" that guide the model to produce the desired output for our applications.
 - Prompt _optimization_ which we can think of as the process of "tuning prompts" to improve the model's performance over time.


## 4.1. 2 Why do we need Prompt Engineering?
So why do we need prompt engineering? It turns out that LLMs are great at generating content, but they don't actually _understand_ the context or meaning of the content they generate. Some related challenges:
 - Model responses are _stochastic_. This means that the same model can produce different results for the same prompt - which creates user confusion.
 - Models can _hallucinate_ responses. This means models can produce responses that are inaccurate, nonsensical, or contradictory to facts - which dilutes user trust.
 - Model capabilities _vary_. Each model has its own quirks, causing the same prompt to be interpreted differently by different models - giving inconsistent user experiences.
 
Prompt engineering is a way to address these challenges, by guiding the model to produce more relevant and reliable results. It is about _optimizing a prompt for a given generative AI model and a desired application goal_ so that we deliver relevant and reliable experiences to our users.
