<!--
LESSON TEMPLATE:
This unit should cover core concept #1.
Reinforce the concept with examples and references.

CONCEPT #1:
Prompt Engineering.
Define it and explain why it is needed.
-->

In this lesson unit, we'll focus on answering two questions:
 1. What is Prompt Engineering?
 2. Why is Prompt Engineering needed?

Let's dive in!


## 4.1.1 What is Prompt Engineering?

We know that Generative AI applications can create new kinds of content (text, images, audio, code and more) in response to a text input (question) from the user. This text input is called a **prompt** and prompt engineering is the **process of designing and optimizing prompts** for Generative AI applications.

Prompts tell the Generative AI model what to do. Think of them almost like a set of _instructions or questions_ that you provide to as a rubric to guide the model towards more relevant and consistent responses. And, just like with any rubric, the _quality_ of the returned response depends on the _clarity and guidelines_ provided by the instructions.

Think of prompt engineering as a 2-part process:
 1. **Prompt design** - the process of "writing a good first prompt" that provides core instructions to guide the model towards producing a _relevant_ response.
 2. **Prompt optimization** - the process of "tuning the prompt" with repeated iterations - validating results each time till the _quality_ of prompt responses meets desired expectations.

The most important thing to remember is that prompt engineering is **more art than science**. Think of it as a trial-and-error process where you can learn and apply recommended techniques (some of which we'll cover in this lesson) first, to improve the quality of results. But you'll also bring your own application domain knowledge and intuition next, to refine the prompt further till you get the results you need.


## 4.1. 2 Why do we need Prompt Engineering?

Just like with rubrics, prompts can benefit from an _iterate and validate_ process where we design the prompt, then see how well those instructions were understood by analyzing the responses, then refine the prompt and try again. Iterate till results are _closer_ to our expectations.

But why do we need an entire **prompt engineering discipline** with tools, techniques and best practices, for use in generative AI applications? Shouldn't our intuition be enough? It's because LLMs are great at _generating content_ but are clueless about the _meaning_ of the content they just created. So they can't tell if the output was relevantand met your expectations for quality. 

Here are some of the challenges that prompt engineering is trying to address better:

1. **Model responses are stochastic.** The same model may produce different results for the same prompt input. This can lead to inconsistent user experiences in your generative AI apps, and have an impact on follow-up actions or workflows driven from it.
1. **Models can hallucinate responses.** The model can return responses that are incorrect, imaginary, or contradictory to known facts. Because LLMs use _pre-trained models_ (based on massive but finite training data) they can lack knowledge about concepts outside that trained scope. And since they don't provide citations for their responses, we have no way of knowing if outputs were valid or not.
1. **Models capabilities vary.** Want to generate a text summary of an article? There are many LLM options to pick from (GPT-4 from OpenAI, LLaMA-2 from Meta, BERT from Google etc.) - but each has its own quirks and features, leading to results that may vary in content relevance, quality, and format. To get consistent or higher-quality results, we need to _fine-tune_ usage to suit the model.

Prompt engineering now becomes a tool and process we can apply, to **guide the model towards relevant, quality results** by applying best practices (e.g. for a given model) in combination with context and intuition (e.g., for a given application domain) to deliver more consistent results that will align better to both user and developer expectations for the generative AI application.



## 4.1.3 Case Study: GitHub Copilot

Want to get a sense of how prompt engineering was applied to build an enterprise-grade generative AI application using a familiar Large Language Model? Say hello to [GitHub Copilot](https://github.com/features/copilot) - your "AI Pair Programmer" that brings the power of chat-based suggestions and support right into your development environment in Visual Studio Code.

- **May 2023** | Learn how GitHub engineers used [prompt engineering](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/) to make the model provide _contextually relevant responses with low latency_.
- **May 2023** | GitHub Copilot used the [OpenAI Codex model](https://openai.com/blog/openai-codex). Learn how engineers then [improved the model itself](https://github.blog/2023-05-17-inside-github-working-with-the-llms-behind-github-copilot/) to improve initial prompt-design, and subsequent prompt-tuning steps - with a focus on _code generation_ as the application domain context.
- **Jun 2023** | GitHub developer advocates released their [How to use GitHub Copilot](https://github.blog/2023-06-20-how-to-write-better-prompts-for-github-copilot/) guide, providing prompt examples and prompt engineering best practices for devs.
- **Jul 2023** | GitHub engineers release [A Developer's Guide to Prompt Engineering and LLMs](https://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms/) that extends beyond basic prompt design to building "prompt engineering pipelines" (_gather context - snippeting - dressing them up - priortization - completion - stop criteria_) for enhanced developer workflows targeting generative AI applications.
- **Sep 2023** | It took 3+ years for GitHub Copilot to go from idea to production. This [How to build an enterprise LLM app: Lessons from GitHub Copilot](https://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/) describes a broad strategy (_find it, nail it, scale it_) for building enterprise-grade solutions like the education-startup AI application we are currently discussing.

Want to keep up with the GitHub Copilot team's learnings? Check out their [Engineering blog](https://github.blog/category/engineering/) for new posts like [this one](https://github.blog/2023-09-27-how-i-used-github-copilot-chat-to-build-a-reactjs-gallery-prototype/) which shows you more examples of usage that you can apply to your own projects.