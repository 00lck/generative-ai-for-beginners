<!--
LESSON TEMPLATE:
This unit should cover core concept #1.
Reinforce the concept with examples and references.

CONCEPT #1:
Prompt Engineering.
Define it and explain why it is needed.
-->

In this unit, we'll focus on answering two questions:
 1. What is Prompt Engineering?
 2. Why is Prompt Engineering needed?

Let's dive in!


## 4.1.1 What is Prompt Engineering?

Generative AI applications take text-based inputs (or prompts) from the user, and respond with a relevant response. In the case of models like GPT3.5 and GPT-4 from OpenAI, the response is a **text completion** reflecting the model's attempt at continuation of the input text or related instruction.

This makes models sensitive to the structure and content of the prompt. Prompts tell the Generative AI model what to do, effectively configuring the model weights to support completion of the desired task. Prompt _engineering_ is the process by which we **guide the model towards more relevant responses** by providing more useful instructions or context. We can approach this as a 2-part process:

 1. **Prompt construction** - the process of "writing a good first prompt" that guides the model towards producing a _relevant_ response as the text completion.
 2. **Prompt optimization** - the process of "refining that prompt" by iterating on the input and validating the output, till it produces a _desired quality_ of that response.

The main thing we need to remember is that prompt engineering is more art than science. It's a trial-and-error process where we can start off by learning and applying best practices for prompt engineering - but then bring in our own intuition and domain-specific knowledge to customize those techniques further till we get results that meet our expectations.


## 4.1. 2 Why do we need Prompt Engineering?

Generative Pre-Trained Transformer (GPT) models like OpenAI's GPT-3 and GPT-4 are powerful - but they are not perfect. Prompts help users "program" models like this to complete a desired task, ideally by giving it instructions or examples to follow. But because prompts use natural language, different users can construct prompts for the same request in different ways, making the model more sensitive to the structure and content in prompts.


But why do we need an entire **prompt engineering discipline** with tools, techniques and best practices, for use in generative AI applications? Shouldn't our intuition be enough? It's because LLMs are great at _generating content_ but are clueless about the _meaning_ of the content they just created. So they can't tell if the output was relevantand met your expectations for quality. 

Here are some of the challenges that prompt engineering is trying to address better:

1. **Model responses are stochastic.** The same model may produce different results for the same prompt input. This can lead to inconsistent user experiences in your generative AI apps, and have an impact on follow-up actions or workflows driven from it.
1. **Models can hallucinate responses.** The model can return responses that are incorrect, imaginary, or contradictory to known facts. Because LLMs use _pre-trained models_ (based on massive but finite training data) they can lack knowledge about concepts outside that trained scope. And since they don't provide citations for their responses, we have no way of knowing if outputs were valid or not.
1. **Models capabilities vary.** Want to generate a text summary of an article? There are many LLM options to pick from (GPT-4 from OpenAI, LLaMA-2 from Meta, BERT from Google etc.) - but each has its own quirks and features, leading to results that may vary in content relevance, quality, and format. To get consistent or higher-quality results, we need to _fine-tune_ usage to suit the model.

Prompt engineering now becomes a tool and process we can apply, to **guide the model towards relevant, quality results** by applying best practices (e.g. for a given model) in combination with context and intuition (e.g., for a given application domain) to deliver more consistent results that will align better to both user and developer expectations for the generative AI application.



## 4.1.3 Case Study: GitHub Copilot

Want to get a sense of how prompt engineering was applied to build an enterprise-grade generative AI application using a familiar Large Language Model? Say hello to [GitHub Copilot](https://github.com/features/copilot) - your "AI Pair Programmer" that brings the power of chat-based suggestions and support right into your development environment in Visual Studio Code.

- **May 2023** | Learn how GitHub engineers used [prompt engineering](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/) to make the model provide _contextually relevant responses with low latency_.
- **May 2023** | GitHub Copilot used the [OpenAI Codex model](https://openai.com/blog/openai-codex). Learn how engineers then [improved the model itself](https://github.blog/2023-05-17-inside-github-working-with-the-llms-behind-github-copilot/) to improve initial prompt-design, and subsequent prompt-tuning steps - with a focus on _code generation_ as the application domain context.
- **Jun 2023** | GitHub developer advocates released their [How to use GitHub Copilot](https://github.blog/2023-06-20-how-to-write-better-prompts-for-github-copilot/) guide, providing prompt examples and prompt engineering best practices for devs.
- **Jul 2023** | GitHub engineers release [A Developer's Guide to Prompt Engineering and LLMs](https://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms/) that extends beyond basic prompt design to building "prompt engineering pipelines" (_gather context - snippeting - dressing them up - priortization - completion - stop criteria_) for enhanced developer workflows targeting generative AI applications.
- **Sep 2023** | It took 3+ years for GitHub Copilot to go from idea to production. This [How to build an enterprise LLM app: Lessons from GitHub Copilot](https://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/) describes a broad strategy (_find it, nail it, scale it_) for building enterprise-grade solutions like the education-startup AI application we are currently discussing.

Want to keep up with the GitHub Copilot team's learnings? Check out their [Engineering blog](https://github.blog/category/engineering/) for new posts like [this one](https://github.blog/2023-09-27-how-i-used-github-copilot-chat-to-build-a-reactjs-gallery-prototype/) which shows you more examples of usage that you can apply to your own projects.