<!--
LESSON TEMPLATE:
This unit should cover core concept #2.
Reinforce the concept with examples and references.

CONCEPT #2:
Prompt Design.
Illustrated with examples.
-->

In the last section, we defined prompt engineering, and described why it is important for generative AI applications. Now, let's dive under the hood to see how prompts are constructed, and understand the implications of various design choices with examples. 

We'll document our prompt (input) and completion (output) for each example  - but you are encouraged to go try these out yourself either with an online Playground site (OpenAI or Azure OpenAI), or by using the Jupyter Notebook provided in the Code Challenge section.

In the next section, we'll look at how _basic prompts are constructed_ using examples to [explain key concepts](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering) including:
 - Prompt Construction (think roles: user, system, assistant)
 - Primary Content (think format: examples, cues, templates)
 - Supporting Content (think tuning: context)


## 4.2.1 Prompt Construction

### Basic Prompt 
Let's start with a basic prompt - this is just a text input that we send to the model with no other context e.g., as shown in the OpenAI [Completion API](https://platform.openai.com/docs/api-reference/completions). _This illustrates the fundamental behavior of LLMs, which is to predict the next word in sequence, based on its training data and parameters_.

| Prompt (Input) | Completion (Output) |
|:---|:---|
| Oh say can you see | It sounds like you're starting the lyrics to "The Star-Spangled Banner," the national anthem of the United States. The full lyrics are ... |


### Complex Prompt

A more complex prompt structure combines the _user_ input (basic prompt) with additional components that can provide additional context and instructions for the model. For instance, the [Chat Completion API](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt) takes input in the form of a collection of _messages_, each associated with one of 3 roles:
 - System - messages that set the _behavior or personality_ for the AI assistant.
 - User - messages that are input by the user, as requests or comments
 - Assistant - messages generated by the AI assistant in response to user

The request is now a multi-part structure [as shown here](https://platform.openai.com/docs/guides/gpt/chat-completions-api) where _prompt engineering_ can involves setting relevant system context to establish a consistent behavior for the AI assistant in a multi-turn conversation.

```python
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
        {"role": "user", "content": "Where was it played?"}
    ]
)
```

### Instruction Prompt

The user input (basic prompt) can involve asking a simple question as shown above - or it can provide more detailed _instructions_ to guide the model response in terms of content, format, length etc.

| Prompt (Input) | Completion (Output) | Instruction Type |
|:---|:---|:---|
| Write a description of the Civil War | _returned a simple paragraph_ | Simple|
|Write a description of the Civil War. Provide key dates and events and describe their significance | _returned a paragraph followed by a list of key event dates with descriptions_| Complex  |
|Write a description of the Civil War in 1 paragraph. Provide 3 bullet points with key dates and their significance. Provide 3 more bullet points with key historical figures and their contributions. Return the output as a JSON file | _returns more extensive details in a text box, formatted as JSON that you can copy-paste to a file and validate as needed_| Complex. Formatted. |

## 4.2.2 Primary Content

In this prompt construction, the input text has _two components_ - a primary content segment, and an instruction that should operate on that content. Here's a simple example of that patttern:

| Prompt (Input) | Completion (Output) |
|:---|:---|
| Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus. <br/> **Summarize this in 2 short sentences** | Jupiter, the fifth planet from the Sun, is the largest in the Solar System and is known for being one of the brightest objects in the night sky. Named after the Roman god Jupiter, it's a gas giant whose mass is two-and-a-half times that of all other planets in the Solar System combined. |

The primary content can be constructed using a variety of techniques. Some common ones are:
 - **Examples** - provide a few examples of the desired output, and let the model infer the pattern.
 - **Cues** - provide a few cues to the desired output, and let the model infer the pattern.
 - **Templates** - provide a template with placeholders for the desired output, and let the model fill in the blanks.

Let's look at these in more detail. 

### Learning Examples

Also known as _in-context learning_ this is an approach where you use the primary content to "feed the model" some examples of the desired output for a given instruction, and let it infer the patter for the desired output. The model will then start using that pattern for its responses. Based on the number of examples provided, we can have zero-shot prompting, one-shot prompting, few-shot prompting etc.

The prompt now consists of three components:
 - A task description
 - A few examples of the desired output
 - The start of a new example (which becomes an implicit task description)


| Learning Type | Prompt (Input) | Completion (Output) |
|:---|:---|:---|
| Zero-shot  | "The Sun is Shining". Translate to Spanish  | "El Sol está brillando".|
| One-shot | "The Sun is Shining" => ""El Sol está brillando". <br> "It's a Cold and Windy Day" => | "Es un día frío y ventoso". |
| Few-shot | The player ran the bases => Baseball  <br/> The player hit an ace => Tennis  <br/> The player hit a six => Cricket  <br/> The player made a slam-dunk => | Basketball |
| | | |

Note how we had to provide an explicit instruction ("Translate to Spanish") in zero-shot prompting, but it gets inferred in the one-shot prompting example. The few-shot example shows how adding more examples allows models to make more accurate inferences with no added instructions.

### Prompt Cues

Another technique for using primary content is to provide _cues_ rather than examples. In this case, we are giving the model a nudge in the right direction by _starting it off_ with a snippet that reflects the desired response format. The model then "takes the cue" to continue in that vein.

| Number of Cues | Prompt (Input) | Completion (Output) |
|:---|:---|:---|
| 0  | Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history.  <br/>**Summarize This**  | Jupiter is the largest planet in our Solar System and the fifth one from the Sun. It is a gas giant with a mass 1/1000th of the Sun's, but it is heavier than all the other planets combined. Ancient civilizations have known about Jupiter for a long time, and it is easily visible in the night sky..|
| 1 | Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history.  <br/>**Summarize This** <br/> What we learned is that Jupiter | is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets combined. It is easily visible to the naked eye and has been known since ancient times. |
| 2 | Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history.  <br/>**Summarize This** <br/> Top 3 Facts We Learned: | 1. Jupiter is the fifth planet from the Sun and the largest in the Solar System. <br/> 2. It is a gas giant with a mass one-thousandth that of the Sun...<br/> 3. Jupiter has been visible to the naked eye since ancient times ... |
| | | |


### Prompt Templates

A prompt template is a _pre-defined recipe for a prompt_ that can be stored and reused as needed, to drive more consistent user experiences at scale. In its simplest form, it is simply a collection of prompt examples like [this one from OpenAI](https://platform.openai.com/examples) that provides both the interactive prompt components (user and system messages) and the API-driven request format - to support reuse.

In it's more complex form like [this example from LangChain](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) it contains _placeholders_ that can be replaced with data from a variety of sources (user input, system context, external data sources etc.) to generate a prompt dynamically. This allows us to create a library of reusable prompts that can be used to drive consistent user experiences **programmatically** at scale.

Finally, the real value of templates lies in the ability to create and publish _prompt libraries_ for vertical application domains - where the prompt template is now _optimized_ to reflect application-specific context or examples that make the responses more relevant and accurate for the targted user audience. The [Prompts For Edu](https://github.com/microsoft/prompts-for-edu) repository is a great example of this approach, curating a library of prompts for the education domain with emphasis on key objectives like lesson planning, curriculum design, student tutoring etc.


## 4.2.3 Supporting Content

If we think about prompt construction as having a instruction (task) and a target (primary content), then _secondary content_ is like additional context we provide to **influence the output in some way**. It could be tuning parameters, formatting instructions, topic taxonomies etc. that can help the model _tailor_ its response to be suit the desired user objectives or expectations.

For example: Given a course catalog with extensive metadata (name, description, level, metadata tags, instructor etc.) on all the available courses in the curriculum:
 - we can define an instruction to "summarize the course catalog for Fall 2023"
 - we can use the primary content to provide a few examples of the desired output
 - we can use the secondary content to identify the top 5 "tags" of interest.

Now, the model can provide a summary in the format show by the few examples - but if a result has multiple tags, it can prioritize the 5 tags identified in secondary content.