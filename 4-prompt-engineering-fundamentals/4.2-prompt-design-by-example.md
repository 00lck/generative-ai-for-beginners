<!--
LESSON TEMPLATE:
This unit should cover core concept #2.
Reinforce the concept with examples and references.

CONCEPT #2:
Prompt Design.
Illustrated with examples.
-->

Before we dive into techniques for effective prompt design, let's look at some examples of prompts for real-world applications. We'll first explore text-based prompts _with the same model_ but different application domains. Then we'll use the same prompt _with different models_ to see how the results vary.

## 4.2.1 Explore: Prompt Examples
At the end of this lesson, we'll teach you how to use a GitHub Codespaces enabled Jupyter Notebook to build and validate prompts for your own projects, with an OpenAI API key. 

For now, let's try these prompts on the [OpenAI Playground](https://beta.openai.com/playground) to see the results - here's what that looks like. Note:
 - This interface allows you to tune _LLM hyperparameters_ like temperature, frequency penalty etc. [Scan this article](https://txt.cohere.com/llm-parameters-best-outputs-language-ai/) for a quick explanation of what they do. We'll cover these in a future lesson.
 - The application indicates that default model training data cuts off in 2021. We'll look at how this impacts the results in one of our examples.

![Open API Playground](./assets/openai-playground.png)

Remember that we are working on an application for an education startup - so let's think about prompts in the context of that application.

## 4.2.2 Example 1: Basic Prompts

Let's start with a simple open-ended question that we might ask a chatbot for our education startup. _The response is relevant - but kinda lengthy.I just want the big picture. Let's refine the prompt!_

![](./assets/effective-prompt-basic.png)

Let's ask it to limit the length to 3 sentencees. _This is much better aligned to what I wanted!_

![](./assets/effective-prompt-refined.png)

This is great - my text input generated a text output. How does it do with code? Let's ask it to generate a Python function for a very well-known problem. _Hmm! I am not a Python programmer but it feels like a valid response, right?_

![](./assets/code-generation-python.png)

## 4.2.3 Example 2: Preset Prompts 

What else can we do with prompts? Turns out we can do many things like _text summarization, content generation, code generation,text transformation_ and more. The [Open AI Examples](https://platform.openai.com/examples) page is a great resource to explore prompts for different application categories.

![](./assets/openai-examples.png)

This one looks interesting - pick the `Summarize for a 2nd grader` example. You can copy this manually into the playground - or you can simply click _Launch Playground_ to get the result.

![](./assets/openai-examples-presets.png)

And here is what that result looks like when you run it in the current OpenAI playground. _Note how this now has content in the `System` and `User` input sections of the application_. This is one technique for effective prompt engineering - providing context with a _System_ prompt, and instructions for the output response with a _User_ prompt.

![](./assets/openai-examples-run.png)

## 4.2.4 Example 3: GitHub Copilot

Now let's look at what happens when we try the same prompts with a _different LLM application_ that is build on a different LLM (or a customized version of it for that domain). We'll use GitHub Copilot for this purpose - if you want to follow along, install the GitHub Copilot Chat extension and activate it now.

Let's try the same basic prompt. Contrast this to the response from the OpenAI playground. Note how the response is not only shorter, but is more technical in tone with focus on _accuracy and relevance_.

![](./assets/effective-prompt-basic-copilot.png)

Let's try refining that prompt like before. Much nicer - also note how the response here contextualizes this with _improve the quality of generated code_ as the first priority.

![](./assets/effective-prompt-refined-copilot.png)

We saw that the _same prompt_ generates different responses with different LLM apps. What if we used the same prompt on the _same LLM app_ for a redo? The response should be the same, right? _Not quite!_ This highlights the stochastic nature of these models, and underscores why prompt engineering is important. To get more consistent results, we need to iteratively _refine_ our prompts to improve the model's performance.

![](./assets/effective-prompt-refined-copilot-redo.png)

Let's try the code example here. _Note how the response is cleaner and promotes reuse, with a focus on readability and maintainability_. GitHub Copilot is tuned for code generation, and we can see the impact in the quality of the response.

![](./assets/code-generation-python-copilot.png)


## 4.2.5 Example 4: Prompts for Edu

TODO: Explore some use cases from repo

## 4.2.6 Summary: What We Learned

TODO: Summarize what we learned from examples

Let's now talk about Prompt Engineering Techniques.
